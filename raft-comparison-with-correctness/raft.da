import sys
import random
import time
import copy
import psutil
import os

# Enums for roles:
class Role: pass
class Follower(Role): pass
class Candidate(Role): pass
class Leader(Role): pass

# The sections denoted in comments/docstrings are from Diego Ongaro's thesis dissertation.
class LogEntry:
    """Fictional log entries."""
    def __init__(self, logType, index, term, client, command, key, value, peerList):
        self.logType = logType
        self.index = index
        self.term = term
        self.client = client
        self.command = command
        self.key = key
        self.value = value
        self.peerList = peerList

    def __str__(self):
        sl = ['LogEntry', str(self.logType), str(self.index), str(self.term), str(self.client), str(self.command), str(self.key), str(self.value), str(self.peerList)]
        return ':'.join(sl)

class Snapshot(process):
    '''
    - initialized by each server process
    - invokes a paralelly running sub-process at each server
    - the sub process takes care of snapshots creation and management
    - runs parallel so as to affect clock the main server process
    '''



    # Implementation is as per Diego Ongaro's thesis dissertation"In Search of an Understandable Consensus Algorithm" section 7 Log Compaction
    def setup():
        #Required to maintain last snapshotted index for this server
        self.prevSnapshotIndex = 0
        #Configuration size for max log length before snapshotting
        self.logThreshold = 4
        self.snapshots = []
        self.proc = psutil.Process(os.getpid())

    def record_memory_consumed():
        send(('memory', self, proc.memory_info()[1]/1000), to= parent())

    #Implementation is similar to Hashicorp's Raft implementation's Snapshotting process https://github.com/hashicorp/raft/blob/master/snapshot.go
    def run():
        #output("PARENT", parent())
        while(True):
            #Checks if the 'incompacted log size' has reached the threshold
            if await(some(received(('log_commitedIndex', commitIndex, log)), has= (commitIndex - prevSnapshotIndex  >= logThreshold))):
                #if true, initiate the takeSnapshot process
                takeSnapshot(log, commitIndex)
            #If it receives an exit command from the server, it shuts down
            elif some(received(('snap_exit'))):
                break



    def takeSnapshot(log, commitIndex):
        '''
        take_snapshot() ->  string, error
        Take_snapshot triggers the action of taking a new snapshot.
        Returns the ID of the new snapshot, along with an error if any
        '''

        compacted = {}

        count = 0
        uncommited = []

        new_snaps = []
        for l in range(prevSnapshotIndex, commitIndex + 1):
            compacted[log[l].key] = log[l].value

        new_snaps.append(LogEntry(logType='SNAPSHOT', index=commitIndex, key=None, value=compacted, term=0, client=None, command=None, peerList=set()))
        #compacted new log with snapshots
        merged = snapshots + new_snaps

        #notify the server of the new compacted log
        send(('snapshot_update', commitIndex, merged), to=parent())

        #if server does not acknowledge snapshot, ignore the snapshot else update prevSnapshotIndex
        if await(some(received(('snapshot_update', ci)), has= (ci == commitIndex))):
            prevSnapshotIndex = ci
            snapshots = merged
            output("INITIAL LOG: ", log)
            output("COMPACTED LOG: ", (snapshots))
        elif timeout(10):
            output("COMPACTION FAILED DUE TO TIMEOUT")
            pass
        record_memory_consumed()

class Server(process):
    def setup(peers, clients, maxTimeout, isPartOfNewConfig):
        self.currentRole = Follower # Tracks server state
        # Persistent states:
        self.currentTerm = 0
        self.currentIndex = 0
        self.votedFor = None
        self.log = [LogEntry(logType='LOG', index=0, key=None, value=None, term=0, client=None, command=None, peerList=set())]
        # Volatile states:
        self.commitIndex = 0
        #store the index of prev snapshot
        self.prevSnapshotIndex = 0
        self.lastApplied = 0
        self.logThreshold = 2
        # Leader states:
        self.nextIndex = dict((p, 1) for p in peers)
        self.matchIndex = dict((p, 0) for p in peers)
        # True if the leader needs to broadcast a heartbeat
        self.has_idled = False
        # no_op_state => -1: Default state, 0: no_op entry is added to the log by Leader, 1: no_op entry is now committed
        self.no_op_state = -1
        # This stores the index of the log entry at which the no_op entry is inserted by the Leader
        self.no_op_index = 0
        # This stores a key value pair of the Client and the latest 'ReadRequest' by that Client. At any point of time, there can only be one Read request by one Client.
        self.latestClientReadRequest = {}
        # Flag to indicate if there are any unserved Read requests
        self.read_requested = False
        # Keeps track of the number of heartbeat responses received. This is for the special heartbeat message triggered by a read-only request.
        self.read_heartbeat_responses_received = 0
        # Flag to indicate if the Leader is currently accepting requests
        self.accepting_requests = True
        # This stores the index of the log entry at which the reconfig entry is inserted by the Leader
        self.reconfigurationIndex = -1
        # reconfigurationState => -1: Default state, 0: Reconfiguration Initiated, 1: Denotes server addition, 2: Denotes server removal
        self.reconfigurationState = -1
        # Store the server that needs to be added. Used only during add reconfiguration
        self.newServer = None
        # Store the server that needs to be removed. Used only during remove reconfiguration
        self.serverToRemove = None
        # To keep a track of the number of rounds of replication for newly added server
        self.logAppendRound = 0
        # Note the time for round 2 of new server log replication
        self.startTime = None
        # If Leader is to be removed as part of the reconfig, this will be the new leader candidate
        self.target_server = None
        # Choose randomized timeout value
        self.termTimeout = random.randint(int(maxTimeout/2), maxTimeout) / 1000
        # Shortcut for informing clients
        self.last_seen_leader = None
        self._dispatch_table = {Follower  : self.follower_term,
                                Candidate : self.candidate_term,
                                Leader    : self.leader_term}
        # Stores pid for Reconfig Server
        self.addMonitor = None
        #store info of the snapshot child process
        self.snapshots = None
        # Uncomment when using optimized version of Read. This keeps track of latest committed values of each key in the log.
        #self.latestCommitedValues = {}
        # Keeps track of clients with pending requests.
        self.clients_with_pending_requests = set()
        self.proc = psutil.Process(os.getpid())

    def record_memory_consumed():
        send(('memory', self, proc.memory_info()[1]/1000), to= parent())


    def run():
        #initialize the snapshot background process
        snapshots = new(Snapshot, num=1)
        setup(snapshots,())
        #time.sleep(5)
        start(snapshots)

        while True:
            if commitIndex > lastApplied:
                lastApplied += 1
                commit_to_state_machine()

            # Choose randomized timeout value for this term:
            #termTimeout = random.randint(int(maxTimeout/2), maxTimeout) / 1000

            # Dispatch based on current server role:
            _dispatch_table[currentRole]()

    def follower_term():
        if await(some(received(('AppendEntries', term, _, _, _, _, _, _)),
                      has= term >= currentTerm)):
            reset("Received")
        elif timeout(termTimeout):
            if isPartOfNewConfig:                                                       # Make this true for the new server that got added
                output("Heartbeat timeout, transitioning to Candidate state.")
                currentRole = Candidate
            """
            else:
                output("########### No Heartbeat timeout since this is a new Server.")
            """
        record_memory_consumed()

    def candidate_term():
        --start_election
        currentTerm += 1
        RequestVoteRPC(target=peers,
                       term=currentTerm,
                       candidateId=self,
                       lastLogIndex=len(log)-1,
                       lastLogTerm=log[-1].term)
        if await(len(setof(p, received(('RequestVoteReply', _currentTerm, True),
                                       from_=p))) > len(peers)/2):
            output("Transitioning to Leader.")
            currentRole = Leader
            # Reinitialize volatile Leader states:
            nextIndex = dict((p, len(log)) for p in peers)
            matchIndex = dict((p, 0) for p in peers)
            has_idled = True    # Force initial heartbeat
            # The below code is implemented using Section 6.4
            # Changing no_op_state to 0 to indicate the insertion of a no_op entry to the log
            no_op_state = 0
            currentIndex += 1
            y = random.randint(0, 10)
            output("## Inserting no-op message ")
            # Inserting a no-op entry into the log at the start of a new term.
            log.append(LogEntry(logType='LOG', index=currentIndex, key='x', value=y, term=currentTerm, client=self, command=-1, peerList=set()))
            # Saving the no_op entry index value to later check if it has been committed or not
            no_op_index = len(log) - 1
        elif some(received(('AppendEntries', term, leader, _, _, _, _, _)),
                  has= term >= currentTerm):
            output("Elected leader:", leader, "Reverting to Follower.")
            currentTerm = term
            currentRole = Follower
        elif timeout(termTimeout):
            output("Election term", currentTerm, "timeout, restarting.")
        record_memory_consumed()

    def leader_term():
        for server, index in nextIndex.items():
            if has_idled or index < len(log):
                # Check if the server is the newly added server. Needed to check if it catching up reasonably
                if server is newServer: # Make this None once reconfiguration is done
                    # If it is an add serber change and it is not the final round(2), increment the round value
                    if reconfigurationState == 0 and logAppendRound < 2:
                        logAppendRound += 1
                    # If it is an add server change and it is the last round, start measuring time taken for this round
                    if reconfigurationState == 0 and logAppendRound == 2:  # Enhacement: Take threshold as input?
                        startTime = time.time()

                # Adding 'heartbeat' field in this call to indicate whether this call is a result of a heartbeat or not.
                AppendEntriesRPC(target=server,
                                 term=currentTerm,
                                 leaderId=self,
                                 prevLogIndex=index-1,
                                 prevLogTerm=log[index-1].term,
                                 entries=log[index:],
                                 leaderCommit=commitIndex,
                                 heartbeat=has_idled)
            # If the newly added server is already up-to date with the leader's logs, it can be added to the cluster
            elif server is newServer and reconfigurationState == 0:
                handleReconfig(True)

        has_idled = False
        if await(currentRole is not Leader):
            return
        elif some(n in range(len(log)),
                  has= (n > commitIndex and
                        len(setof(i, i in matchIndex, matchIndex[i] >= n)) >
                        len(peers) / 2 and
                        log[n].term == currentTerm)):
            debug("Updating commitIndex from %d to %d" % (commitIndex, n))
            commitIndex = n
            # Uncomment when using optimized version of Read.
            #latestCommitedValues[log[commitIndex].key] = log[commitIndex].value

            # Check if a no_op entry was initiated and that the no_op entry has been committed
            # If so, update the no_op_state to indicate that the no_op entry is now committed
            if (no_op_state == 0 and commitIndex >= no_op_index):
                no_op_state = 1
            #send current commit index to snapshot process for tracking
            send(('log_commitedIndex', commitIndex, log), to=snapshots)
            """
            if(commitIndex - prevSnapshotIndex > logThreshold):     ############################
                output("BREACH")
                send(('log_snapshot', commitIndex, log), to=snapshots)

            """

            # If the change was remove server and the commitIndex is higher than index at which config was appended, it has been committed by a majority of servers.
            # Leader then sends a success message to reconfigServer process so that it can proceed to the next config change.
            # Reset serverToRemove to None
            if reconfigurationState == 2 and commitIndex >= reconfigurationIndex:
                output("## Remove Server : New configuration successfully committed to majority.")
                send(('RemoveServerComplete', serverToRemove, True), to= addMonitor)
                reconfigurationState = -1
                serverToRemove = None

            # If the change was add server and the commitIndex is higher than index at which config was appended, it has been committed by a majority of servers.
            # Leader then sends a success message to reconfigServer process so that it can proceed to the next config change.
            # Reset newServer to None
            if reconfigurationState == 1 and commitIndex >= reconfigurationIndex:
                output("## Add Server : New configuration successfully committed to majority.")
                send(('AddServerComplete', newServer, True), to= addMonitor)
                reconfigurationState = -1
                newServer = None

        # Idle timeout is half of normal term timeout:
        elif timeout(termTimeout/2):
            debug("Idle timeout triggered.")
            has_idled = True

        record_memory_consumed()

    def handleReadOnlyHeartbeatResponse():
        """ Helper method to handle the scenario where a heartbeat was successfully acknowledged for a read only request """
        # Processs further only if a no_op entry is committed (indicated by the no_op_state value as 1) and a read request is not served yet
        if read_requested:
            read_heartbeat_responses_received += 1
            #output("## Current Size is: ", read_heartbeat_responses_received)
            if no_op_state == 1 and read_heartbeat_responses_received > len(peers) / 2:
                output("## Checking for an existing entry in the logs: ", read_heartbeat_responses_received)
                # Traverse the log starting from the latest entry.
                # If 1. Current log index is lesser than the current commit index
                #    2. Client value in the current log is available in 'latestClientReadRequest' (indicating that this Client has a pending read request)
                #.   3. Key value in the current log is the key whose value this Client is requesting
                # Then send the value of this key present in the current log and delete this clients request from 'latestClientReadRequest' since this request is served.
                for i in range(len(log)-1,-1,-1):
                    if i <= commitIndex and log[i].client in latestClientReadRequest and latestClientReadRequest[log[i].client].key == log[i].key:
                        send(('ReadReply', latestClientReadRequest[log[i].client].serial, True, log[i].value), to=log[i].client)
                        del latestClientReadRequest[log[i].client]

                # Optimization to above loop
                #for i in range(commitIndex, -1, -1): # Can use this instead to avoid the check for i <= commitIndex
                """
                # We can optimize the above loop by instead using the below logic. This runs for each read request only. Above loop runs through the entire log.
                for requesting_client in latestClientReadRequest.keys():
                    #output("## Current client : ", requesting_client)
                    currect_key = latestClientReadRequest[requesting_client].key
                    if currect_key not in latestCommitedValues:
                        #output("## Current client : Not found key: ", latestClientReadRequest[requesting_client].key)
                        send(('ReadReply', latestClientReadRequest[requesting_client].serial, False, -1), to=requesting_client)
                    else:
                        #output("## Current client : Found key: ", latestClientReadRequest[requesting_client].key)
                        send(('ReadReply', latestClientReadRequest[requesting_client].serial, True, latestCommitedValues[currect_key]), to=requesting_client)
                """

                # For all the remaining requests in 'latestClientReadRequest' return a False response to the appropriate client to indicate that the key was not found.
                for entry in latestClientReadRequest.keys():
                    send(('ReadReply', latestClientReadRequest[entry].serial, False, -1), to=entry)        # Update no_op_state to -1 here? Command/serial is not known here!!

                # Change this to False to indicate that all read requests are now served.
                read_requested = False
                read_heartbeat_responses_received = 0

        record_memory_consumed()


    def receive(msg= ('snapshot_update', lastIndex, updateLogs)):
        #acknowledge new snapshot created
        output(" NEW SNAPSHOT RECEIVED")
        send(('snapshot_update', lastIndex), to=snapshots)
        record_memory_consumed()
        #--snapshots

    def handleReconfig(canAddServer):
        """Method to handle addition of server. If the leader determines that new server catches up reasonably well, addition is initiated. Otherwise, it is aborted.
        Section 4.1 : Cluster configurations are stored and communicated using special entries in the replicated log. This leverages the existing mechanisms in Raft to replicate and persist configuration information. """
        if not canAddServer:
            # State denotes that reconfiguration is complete
            reconfigurationState = -1
            # Inform reconfigServer process that add server change will be aborted
            send(('AddServerComplete', newServer, False), to= addMonitor)
            newServer = None
        else:
            # State denotes that change being carried out is addServer
            reconfigurationState = 1
            currentIndex += 1
            # Add the log entry for reconfig change. Special entry of the type "CONFIG"
            log.append(LogEntry(logType= "CONFIG", index= currentIndex, term=currentTerm, client=addMonitor, command=newServer, key=None, value=None, peerList=peers))
            # Maintain index to check when reconfig log entry has been committed by majority
            reconfigurationIndex = len(log) - 1
        record_memory_consumed()

    def receive(msg= ('RequestVote', term, candidateId,
                      lastLogIndex, lastLogTerm)):
        update_term(term)
        if term < currentTerm:
            RequestVoteReply(target=candidateId,
                             term=currentTerm, voteGranted=False)
        elif ((votedFor is None or votedFor == candidateId) and
              is_up_to_date(lastLogIndex, lastLogTerm)):
            votedFor = candidateId
            RequestVoteReply(target=candidateId,
                             term=currentTerm, voteGranted=True)
        else:
            RequestVoteReply(target=candidateId,
                             term=currentTerm, voteGranted=False)
        record_memory_consumed()

    def receive(msg= ('RequestVoteReply', term, False)):
        update_term(term)
        record_memory_consumed()

    def receive(msg= ('AppendEntries', term, leaderId, prevLogIndex, prevLogTerm,
                      entries, leaderCommit, has_idled)):
        update_term(term)
        if term < currentTerm:
            AppendEntriesReply(target=leaderId,
                               term=currentTerm, success=False, heartbeat=False)
        elif not (len(log) > prevLogIndex and
                  log[prevLogIndex].term == prevLogTerm):
            AppendEntriesReply(target=leaderId,
                               term=currentTerm, success=False, heartbeat=False)
        else:
            last_seen_leader = leaderId
            for idx, entry in enumerate(entries):
                # Section 4.1 Config changes are considered to be special type of log entries. Whenever a server receives it, they update their configuration and function based on the new config.
                if entry.logType == "CONFIG":
                    peers.clear()
                    # New peer list with the necessary addition/removal
                    for item in entry.peerList:
                        peers.add(item)

                idx += prevLogIndex + 1
                if len(log) <= idx:
                    log.append(entry)
                elif log[idx].term != entry.term:
                    del log[idx:]
            last_new_index = prevLogIndex + len(entries)
            if leaderCommit > commitIndex:
                commitIndex = min(leaderCommit, last_new_index)
            AppendEntriesReply(target=leaderId,
                               term=currentTerm, success=True,
                               updatedIndex=last_new_index,
                               heartbeat=has_idled)
        record_memory_consumed()

    def receive(msg= ('AppendEntriesReply', term, success, updatedIndex, heartbeat),
                from_=server):
        update_term(term)
        # Further action is only needed if we are still leader:
        if currentRole is Leader:
            if server == newServer:
                # Condition number 2 to check if the new server is sufficiently caught up.
                # Section 4.2.1 : The leader should also abort the change if the new server is unavailable or is so slow that it will never catch up.
                # Replication of log entries is done in rounds (we consider 2 as maximum).
                # If the last round lasts less than an election timeout, then the leader adds the new server to the cluster, under the assumption that there are not enough unreplicated entries to create a significant availability gap.
                if reconfigurationState == 0 and logAppendRound == 2:
                    diffTime = time.time() - startTime # Paper also has another condition to check!!!
                    if diffTime <= termTimeout:
                        output("## Reconfiguration Server : New server is sufficiently caught up. Can be added successfully now.")
                        handleReconfig(True)
                    else:
                        output("## Reconfiguration : New Server will not be added.")
                        handleReconfig(False)

            if success:
                nextIndex[server] = updatedIndex + 1
                matchIndex[server] = updatedIndex
                # In case of leader removal, we check if the target server has caught up with the leader's log. If yes, we leader triggers a timeout on this target server so that it can become the new leader.
                # Section 3.10
                if not accepting_requests and server == target_server and matchIndex[server] == len(log) - 1:
                    output("## Send timeout to new leader candidate ( target server )")
                    send(('TimeoutNow'), to= server)
            else:
                # Failed because of log inconsistency:
                nextIndex[server] -= 1

            if heartbeat:
                # Helper method to handle the scenario where a heartbeat was successfully acknowledged for a read only request.
                handleReadOnlyHeartbeatResponse()
        record_memory_consumed()

    def receive(msg= ('snap_exit')):
        #notify the snapshot process to end, for the system to shutdown
        send(('snap_exit'), to=snapshots)
        record_memory_consumed()

    def receive(msg= ('TimeoutNow')):
        currentRole = Candidate
        record_memory_consumed()

    def receive(msg= ('ClientRequest', serial, read_only, key, value), from_=client):
        # Added a new boolean variable 'read_only' to indicate whether the current request is a readonly request of not.
        if currentRole is not Leader:
            send(('NotLeader', serial, last_seen_leader), to=client)
        else:
            # Proceed further only if the Leader is accepting requests.
            if accepting_requests is True:
                # Introduced a separate handling for Read-only requests
                if (read_only):
                    output("## Handling Read Only Request")
                    # Forcing heartbeat. (Section 6.4 Point 3)
                    has_idled = True
                    # Saving the latest Read request from the client
                    latestClientReadRequest[client] = ReadRequest(serial=serial, key=key)
                    # Updating boolean variable 'read_requested' to True to indicate that a Read has been requested
                    read_requested = True
                else:
                    currentIndex += 1
                    y = value #random.randint(0, 10)
                    if client in clients_with_pending_requests:     # Optimization for Linearizable Semantics. Check further only if this client has a pending request.
                        for i in range(len(log)-1,-1,-1):
                        # Here, check if this command is already received, if so, then return result of the original command (New feature. Put content from paper)
                            if log[i].client == client:
                                if log[i].command == serial:
                                    if i <= commitIndex:
                                        send(('Reply', serial, self), to=client)
                                        clients_with_pending_requests.remove(client)
                                        return
                                    else:
                                        return
                                else:
                                    break
                    log.append(LogEntry(logType='LOG', index=currentIndex, key='x', value=y, term=currentTerm, client=client, command=serial, peerList=set()))
                    clients_with_pending_requests.add(client)
        record_memory_consumed()

    def handle_leader_removal():
        """Section 3.10
        1. The prior leader stops accepting new client requests.
        2. The prior leader fully updates the target server’s log to match its own, using the normal log replication mechanism described in Section 3.5.
        3. The prior leader sends a TimeoutNow request to the target server.
        This request has the same effect as the target server’s election timer firing: the target server starts a new election (incrementing its term and becoming a candidate).
        Step 2 is carried out normally in leader_term
        step 3 is done in the AppendEntriesReply method
        """
        accepting_requests = False
        while True:
            newLeaderCandidate = random.choice(list(peers))
            if newLeaderCandidate != self :
                target_server = newLeaderCandidate
                output("## New Leader Candidate ( target server )", target_server)
                break
        record_memory_consumed()

    def handle_server_removal(server_to_remove):
        """ Method removes a server from a cluster. The log entry for the CONFIG change triggers the removal in other processes.
        On receipt of the particular entry, servers immediately shift to the new config.
        Section 4.1 : Cluster configurations are stored and communicated using special entries in the replicated log. This leverages the existing mechanisms in Raft to replicate and persist configuration information. """
        send(('snap_exit'), to=server_to_remove)
        serverToRemove = server_to_remove
        send(('ClearPeers'), to= server_to_remove)
        # Remove this server from Leader's peer list
        peers.remove(server_to_remove)
        # Delete it's index state values
        del nextIndex[serverToRemove]
        del matchIndex[serverToRemove]
        # State 2 : denoteds server removal
        reconfigurationState = 2
        currentIndex += 1
        # Add the log entry for reconfig change. Special entry of the type "CONFIG"
        log.append(LogEntry(logType="CONFIG", index=currentIndex, term=currentTerm, client=addMonitor, command=server_to_remove, key=None, value=None, peerList=peers))
        # Index value maintained to check when the config log entry has been committed by majority
        reconfigurationIndex = len(log) - 1
        record_memory_consumed()

    def receive(msg= ('ClearPeers')):
        """ The server to be removed is not considered a part of the majority. We clear it's peer list and force it to quit."""
        peers.clear()
        record_memory_consumed()

    def receive(msg= ('RemoveServer', server_to_remove), from_=reconfig_server):
        """ Receive the removeServer Request from ReconfigServer. If this server is not a Leader, it responds with the last known Leader.
        If the server is a Leader, it triggers the reconfig change.
        Section 4.1 : Reply Not Leader if not a leader """
        if currentRole is not Leader:
            output("## Remove Server request received by Non-Leader for Server: ", server_to_remove)
            send(('NotLeader', server_to_remove, last_seen_leader), to=reconfig_server)
        else:
            output("## Remove Server request received by Leader for Server: ", server_to_remove)
            # Save the ReconfigServer process id to send a success/fail message at the end
            addMonitor = reconfig_server
            # If the server to be removed is the leader itself, it needs to take a few additional steps. Check the method handle_leader_removal()
            if server_to_remove == self:
                output("## Need to remove Leader", server_to_remove)
                handle_leader_removal()
            else:
                output("## Remove Non-Leader ", server_to_remove)
                # If server is not a leader, follow the normal flow
                handle_server_removal(server_to_remove)
        record_memory_consumed()

    def receive(msg= ('AddServers', new_server), from_=addServer):
        """ Receive the addServer Request from ReconfigServer. If this server is not a Leader, it responds with the last known Leader.
        If the server is a Leader, it triggers the reconfig change.
        Section 4.1 : Reply Not Leader if not a leader
        Before the new server can be added, the leader checks that it is catching up with a reasonable speed. This is done in leader_term
        """
        if currentRole is not Leader:
            output("## Reconfiguration received by Non Leader", new_server)
            send(('NotLeader', new_server, last_seen_leader), to=addServer)
        else:
            output("## Reconfiguration received by Leader for Server: ", new_server)
            newServer = new_server
            # Save the ReconfigServer process id to send a success/fail message at the end
            addMonitor = addServer
            # Add the new server to the Leader's peer list.
            peers.add(newServer)
            # Update nextIndex and MatchIndex
            nextIndex[newServer] = 1
            matchIndex[newServer] = 0
            # State 0 denoted that reconfiguration has been initiated
            reconfigurationState = 0
        record_memory_consumed()

    def update_term(term):
        if currentTerm < term:
            currentTerm = term
            votedFor = None
            currentRole = Follower
        record_memory_consumed()

    def is_up_to_date(lastLogIndex, lastLogTerm):
        record_memory_consumed()
        return (lastLogTerm, lastLogIndex) >= (log[-1].term, len(log)-1)

    def commit_to_state_machine():
        entry = log[lastApplied]
        output(entry, " at index", lastApplied, "applied to state machine.")
        if currentRole is Leader:
            send(('Reply', entry.command, self), to=entry.client)
            # Optimization for Linearizable Semantics
            if entry.client in clients_with_pending_requests:
                clients_with_pending_requests.remove(entry.client)
        record_memory_consumed()

    def AppendEntriesRPC(target, term, leaderId, prevLogIndex, prevLogTerm,
                         entries, leaderCommit, heartbeat):
        send(('AppendEntries', term, leaderId, prevLogIndex, prevLogTerm,
              entries, leaderCommit, heartbeat), to=target)
        record_memory_consumed()

    def AppendEntriesReply(target, term, success, heartbeat, updatedIndex=None):
        """ FIXME: 'updatedIndex' is not in original algorithm!
        We need this additional information to pair the reply with the
        original RPC request """
        send(('AppendEntriesReply', term, success, updatedIndex, heartbeat), to=target)
        record_memory_consumed()

    def RequestVoteRPC(target, term, candidateId, lastLogIndex, lastLogTerm):
        send(('RequestVote', term, candidateId, lastLogIndex, lastLogTerm),
             to=target)
        record_memory_consumed()

    def RequestVoteReply(target, term, voteGranted):
        send(('RequestVoteReply', term, voteGranted), to=target)
        record_memory_consumed()

class ReadRequest:
    """ Used to store the request identifier(serial) and the key provided as part of a read request """
    def __init__(self, serial, key):
        self.serial = serial
        self.key = key

class ReconfigServer(process):
    """ Process to carry out any reconfiguration changes. Currently, the change is triggered after 10 seconds. Reconfiguration change is given as a list of operations to be carried out.
    Raft Dissertation Thesis 4.1 : Raft restricts the types of changes that are allowed: only one server can be added or removed from the cluster at a time.
    More complex changes in membership are implemented as a series of single-server changes."""

    def setup(n, oldServers, clients, send_failrate, maxTimeout):
        self.addedServers = set()
        self.proc = psutil.Process(os.getpid())

    def record_memory_consumed():
        send(('memory', self, proc.memory_info()[1]/1000), to= parent())

    def remove_server(server_to_remove):
        """ Emulates Remove Server RPC
        1. Receives not leader if reconfig request sent to some other server. Sends request to the leader then.
        2. Waits for a fail or success message from leader
        3. If successful, sends a message to client to update it's server list
        4. If removal is called on a cluster of 3 servers, it is rejected as it would affect the cluster availability
        """
        output("## Server to remove: ", server_to_remove)
        target = random.choice(list(oldServers))
        while True:
            send(('RemoveServer', server_to_remove), to=target)
            if await(some(received(('NotLeader', _server_to_remove, leader)),
                          has= leader is not None)):
                output("## Reconfig request sent to wrong server, sending to leader", leader)
                target = leader
                reset("Received")

            elif some(received(('RemoveServerComplete', _server_to_remove, status))):
                if status:
                    output("## Remove Server request for Server: ", server_to_remove, " executed successfully.")
                    oldServers.remove(server_to_remove) #Delete the removed server from the serverList
                    send(('DeleteServer', server_to_remove), to= clients)
                    break
                else:
                    output("## Remove Server request for Server: ", server_to_remove, " could not be removed eventually. Retrying")

            elif timeout(20):
                # We need to have a timeout here since the leader might crash and this server will be waiting forever for the result!!
                target = random.choice(list(oldServers))
        record_memory_consumed()

    def add_servers():
        """ Emulates Add Server RPC
        Spawns a new server to be added to the cluster.  Sends request to the leader then.
        1. Receives not leader if reconfig request sent to some other server.
        2. Waits for a fail or success message from leader
        3. If successful, sends a message to client to update it's server list
        4. If the new server could not catch up reasonably to the leader, abort the reconfig change.
        """
        timeout = maxTimeout
        output("## Add new servers")
        for i in range(n):
            server = new(Server, num= 1, send= send_failrate)
            currentServer = copy.deepcopy(list(server)[0])
            oldServers.add(currentServer)
            addedServers.add(currentServer)
            # Set up and start new servers
            setup(server, (oldServers, clients, maxTimeout, False))
            start(server)
            # Send the reconfig request to some server with the list of the additional servers
            target = random.choice(list(oldServers))
            while True:
                send(('AddServers', currentServer), to=target)
                if await(some(received(('NotLeader', _currentServer, leader)),
                              has= leader is not None)):
                    output("## Reconfig request sent to wrong server, sending to leader", leader)
                    target = leader
                    reset("Received")

                elif some(received(('AddServerComplete', _currentServer, status))):
                    if status:
                        output("## Reconfiguration change : Server ", currentServer, " added successfully.")
                        send(('NewServer', server), to= clients)
                    else:
                        output("## Reconfiguration change: Server ", currentServer, " could not be added.")
                    break

                elif timeout(20):
                    # We need to have a timeout here since the leader might crash and this server will be waiting forever for the result!!
                    target = random.choice(list(oldServers))
        record_memory_consumed()

    def run():
        """Triggers reconfiguration changes after 10 seconds.
        Section 4.1 : Before moving on to the next configuration, we wait for the previous configuration to be successful."""
        if await(False):
            pass
        elif(timeout(10)):
            pass
        # Configuration change list
        reconfiguartion_change_list = ['add','remove']
        for change in reconfiguartion_change_list :
            if change == 'add':
                add_servers()
            else :
                # If the cluster currently has only 3 servers, do not go ahead with remove.
                if len(oldServers)>3:
                    server_to_remove = random.choice(list(oldServers))
                    remove_server(server_to_remove)
                else :
                    output("## Removing a server might affect availability. Aborting this change.")
        # Send a message to the parent on reconfig change success
        send(('Reconfiguration'), to=parent())
        # Wait for the parent to send a kill request for the newly added servers
        await(some(received(('KillAll'))))
        # Send message to terminate the snapshot processes for the newly added servers
        send(('snap_exit'), to=addedServers)
        # Terminate the newly added servers
        end(addedServers)
        # Send a done signal to the parent
        send(('DoneMonitor',), to=parent())

class Client(process):
    def setup(servers, nrequests, timeout):
        self.proc = psutil.Process(os.getpid())

    def record_memory_consumed():
        send(('memory', self, proc.memory_info()[1]/1000), to= parent())

    def receive(msg= ('DeleteServer', server)):
        """ Client receives a message from the ReconfigServer process when server removal is successful. Client then deletes the server from it's server list.
        When a server is removed, it is no longer a part of majority. It would be futile for the client to send a request to this server. """
        if server in servers:
            servers.remove(server)
        output("## Update Client list", servers)
        record_memory_consumed()

    def receive(msg= ('NewServer', server)):
        """ Client receives a message from the ReconfigServer process when server addition is successful. It adds the new server to it's server list."""
        servers.extend(list(server)) #server is a set
        output("## Update Client list", servers)
        record_memory_consumed()

    def run():
        target = random.choice(servers)
        req = 0
        key = 'x'

        while req < nrequests:
            send(('ClientRequest', req, False, key, req), to=target)
            if await(some(received(('NotLeader', _req, leader)),
                          has= leader is not None)):
                debug("Wrong server, changing to", leader)
                target = leader
                reset("Received")
            elif some(received(('Reply', _req, _))):
                output("Request", req + 1, "complete.")
                req += 1
            elif timeout(timeout/1000):
                target = random.choice(servers)

        i = 0
        while i < nrequests:
            # Send Read requests to a Server with the key whose value is to be read.
            send(('ClientRequest', i, True, key, -1), to=target)
            # Wait until either you receive
            # 1. 'NotLeader' response for the current request identifier or
            # 2. 'ReadReply' containing the response from the Leader or
            # 3. Time out and retry the same request again with a different Server as target.
            if await(some(received(('NotLeader', _req, leader)),
                          has= leader is not None)):
                debug("Wrong server, changing to", leader)
                # Client sent the Read request to a non-Leader. Re-send the request to the leader provided in response.
                target = leader
                reset("Received")
            elif some(received(('ReadReply', _i, res, value))):
                output("## Read Result", res, " ", key, " = ", value)
                # Client received the response from the Leader. Try the next request.
                i += 1
            elif timeout(timeout/100):
                target = random.choice(servers)

        send(('Done',), to=parent())

def main():
    nservers = int(sys.argv[1]) if len(sys.argv) > 1 else 3
    nclients = int(sys.argv[2]) if len(sys.argv) > 2 else 1
    nrequests = int(sys.argv[3]) if len(sys.argv) > 3 else 10
    maxtimeout = int(sys.argv[4]) if len(sys.argv) > 4 else 5000
    send_failrate = float(sys.argv[5]) if len(sys.argv) > 5 else 0.0

    servers = new(Server, num= nservers, send= send_failrate)
    clients = new(Client, num= nclients)
    setup(clients, (list(servers), nrequests, maxtimeout))
    setup(servers, (servers, clients, maxtimeout, True))
    start(servers)
    start(clients)

    # For reconfiguration, currently, the ReconfigServer process starts the reconfig change after 10 seconds.
    # This can be changed and integrated with a driver class to make it more dynamic.
    # Process that spawns new servers in addition to the exisiting servers/ removes servers
    addMonitor = new(ReconfigServer,(1, servers, clients, send_failrate, maxtimeout))
    start(addMonitor)
    # Wait for the Reconfiguration Changes to be completed
    await(some(received(('Reconfiguration'))))
    output("## Reconfiguration response received by Main Thread")
    # Terminate the monitor class
    await(each(c in clients, has=received(('Done',), from_=c)))
    output("All clients done.")
    send(('snap_exit'), to=servers)
    # Terminate the newly added servers
    send(('KillAll'), to= addMonitor)
    end(servers)
    # Wait for the Done signal from the ReconfigServer process
    await(some(received(('DoneMonitor',))))
    end(addMonitor)
